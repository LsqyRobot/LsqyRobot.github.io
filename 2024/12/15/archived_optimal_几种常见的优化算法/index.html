<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/favicon.ico"/>
	<link rel="shortcut icon" href="/img/favicon.ico">
		<!-- Modified by lsqyRobot -->
    <title>
    Beyond The Memory
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="google-site-verification" content="Mbr82TZaQktpHzJ9Wt0AZ4B9JFdVUi1Xjxs8gf20DDM" />
    <link rel="stylesheet" href="/css/lsqy_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="mathematica,robotics,manipulator,ros,vim,matlab,C/C++,python,git,control theory,lsqyRobot" />
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/myCover.png') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

	    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/dytitle.js"></script>
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <script src="/js/codeBlockFuction.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>
		


<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<body class="is-loading">
    <div id="wrapper" class="fade-in">
        <header id="header">
    <a href="/" class="logo">dots</a>
</header>

        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow"><img src="/img/home.png"></a>
	        </li>

			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1"><img src="/img/folder.png"></a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/ALGORITHM/">ALGORITHM</a></li><li><a class="category-link" href="/categories/C/">C++</a></li><li><a class="category-link" href="/categories/CONFIG/">CONFIG</a></li><li><a class="category-link" href="/categories/CONTROL/">CONTROL</a></li><li><a class="category-link" href="/categories/CPP/">CPP</a></li><li><a class="category-link" href="/categories/GIT/">GIT</a></li><li><a class="category-link" href="/categories/HEXO/">HEXO</a></li><li><a class="category-link" href="/categories/LATEX/">LATEX</a></li><li><a class="category-link" href="/categories/LIFE/">LIFE</a></li><li><a class="category-link" href="/categories/LINUX/">LINUX</a></li><li><a class="category-link" href="/categories/MAC-BOOK/">MAC BOOK</a></li><li><a class="category-link" href="/categories/MATLAB/">MATLAB</a></li><li><a class="category-link" href="/categories/PYTHON/">PYTHON</a></li><li><a class="category-link" href="/categories/ROBOTICS/">ROBOTICS</a></li><li><a class="category-link" href="/categories/ROS/">ROS</a></li><li><a class="category-link" href="/categories/TOOLS/">TOOLS</a></li><li><a class="category-link" href="/categories/dataStruct/">dataStruct</a></li><li><a class="category-link" href="/categories/robotics/">robotics</a>
	                    </ul>
	        </li>
	        


	        <!-- about 关于我   --> 
			<li>
	        
	            <a href="/about/" ><img src="/img/about.png"></a>
	        
			</li>



	        <!-- about 关于我的朋友   --> 
			<li>
	        
	            <a href="/friend/" ><img src="/img/friend.png"></a>
	        
			</li>




	        <!-- about 关于格言   --> 
			<li>
	        
	            <a href="/quote/" ><img src="/img/quote.png"></a>
	        
			</li>


	        <!-- about 关于搜索栏   --> 
			<li>
	        
			</li>



	        <!-- my lover--> 
			<li>
	        
	            <a href="/dudu/" ><img src="/img/myLoverLogo.png"></a>
	        
			</li>

	        <!-- my tags--> 
			<li>
	        
			</li>


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
			</ul>
</nav>


        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(/img/postImg/16_Algorithm/logo/ladybugs-ladybirds-bugs-insects-preview_used.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >常见的优化算法</h2></a>
            </div>

            <div class="typo" style="padding: 3rem;">
                <!-- vim-markdown-toc GFM -->
<ul>
<li><a href="#前言">前言</a></li>
<li><a href="#梯度下降法">梯度下降法</a><ul>
<li><a href="#描述">描述</a></li>
<li><a href="#例子">例子</a></li>
<li><a href="#缺点">缺点</a></li>
<li><a href="#批量梯度下降法">批量梯度下降法</a></li>
<li><a href="#随机梯度下降">随机梯度下降</a></li>
</ul>
</li>
<li><a href="#牛顿法和拟牛顿法">牛顿法和拟牛顿法</a><ul>
<li><a href="#牛顿法">牛顿法</a><ul>
<li><a href="#方法说明">方法说明</a></li>
<li><a href="#牛顿法的优缺点总结">牛顿法的优缺点总结：</a></li>
</ul>
</li>
<li><a href="#拟牛顿法">拟牛顿法</a><ul>
<li><a href="#搜索极值">搜索极值</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#共轭梯度法">共轭梯度法</a><ul>
<li><a href="#方法的表述">方法的表述</a></li>
<li><a href="#算法">算法</a></li>
</ul>
</li>
<li><a href="#序列二次规划">序列二次规划</a><ul>
<li><a href="#sqp方法的基本步骤">SQP方法的基本步骤</a><ul>
<li><a href="#目标函数和约束的泰勒展开">目标函数和约束的泰勒展开</a></li>
<li><a href="#构造二次规划子问题">构造二次规划子问题</a></li>
<li><a href="#解二次规划子问题">解二次规划子问题</a></li>
<li><a href="#判断收敛性">判断收敛性</a></li>
<li><a href="#约束处理">约束处理</a></li>
</ul>
</li>
<li><a href="#sqp的优点">SQP的优点</a></li>
<li><a href="#sqp的缺点">SQP的缺点</a></li>
<li><a href="#应用领域">应用领域</a></li>
</ul>
</li>
<li><a href="#参考">参考</a></li>
</ul>
<!-- vim-markdown-toc -->
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最优化方法是一种数学方法，它是研究在给定约束之下如何寻求某些因素(的量)，以使某一(或某些)指标达到最优的一些学科的总称。</p>
<p>常见的优化算法有很多，涵盖了数学、工程、机器学习等多个领域。以下是一些常见的优化算法的概述：</p>
<p>1) <code>梯度下降法(Gradient Descent)</code></p>
<p>原理：通过计算目标函数的梯度(偏导数)来更新参数，使得目标函数最小化。梯度下降法的基本思想是沿着梯度的反方向更新参数，以最快的速度减少目标函数值。</p>
<p>种类：</p>
<p>&bull; 批量梯度下降(Batch Gradient Descent)：每次迭代计算整个数据集的梯度，适合数据量小的情况。</p>
<p>&bull; 随机梯度下降(Stochastic Gradient Descent, SGD)：每次迭代使用一个样本来计算梯度，适合数据量大的情况。</p>
<p>&bull; 小批量梯度下降(Mini-batch Gradient Descent)：每次迭代使用数据集的一个小批量样本，结合了批量和随机梯度下降的优点。</p>
<p>2) <code>牛顿法(Newton&#39;s Method)</code></p>
<p>原理：利用目标函数的二阶导数(即Hessian矩阵)来加速优化过程。与梯度下降法相比，牛顿法每次更新时考虑了函数的曲率，因此收敛速度较快，但计算代价较高。</p>
<p>应用：适用于目标函数二阶导数可计算且问题规模较小的情况。</p>
<p>3) <code>拟牛顿法(Quasi-Newton Methods)</code></p>
<p>原理：拟牛顿法不直接计算Hessian矩阵，而是通过近似的方法来估算它，从而降低计算成本。最著名的拟牛顿法是BFGS(Broyden–Fletcher–Goldfarb–Shanno)算法。</p>
<p>应用：适用于中等规模的优化问题，尤其是在计算Hessian矩阵过于昂贵时。</p>
<p>4) <code>共轭梯度法(Conjugate Gradient Method)</code></p>
<p>原理：对于二次优化问题，共轭梯度法是一种迭代优化方法，它通过构造一组共轭方向来减少迭代次数。每一步的方向都是与之前的搜索方向“共轭”的。</p>
<p>应用：常用于大型稀疏线性系统的优化问题，尤其适用于内存限制较大的情境。</p>
<p>5) <code>启发式优化算法</code></p>
<p>a) <code>遗传算法(Genetic Algorithm, GA)</code></p>
<p>原理：遗传算法是一种基于自然选择和遗传机制的全局优化方法，通过模拟生物的进化过程来寻找最优解。基本操作包括选择、交叉、变异等。</p>
<p>应用：适用于搜索空间复杂、目标函数非线性且没有明显数学形式的优化问题。</p>
<p>b) <code>粒子群优化(Particle Swarm Optimization, PSO)</code></p>
<p>原理：粒子群优化是一种模拟鸟群觅食行为的优化算法。每个“粒子”代表一个潜在解，粒子通过个体历史最优位置和全体群体历史最优位置来更新自己的位置。</p>
<p>应用：适用于全局优化问题，特别是高维、多峰值问题。</p>
<p>c) <code>模拟退火(Simulated Annealing, SA)</code></p>
<p>原理：模拟退火算法模仿物理学中的退火过程，通过控制“温度”逐渐降低，从而在搜索空间中找到全局最优解。它允许接受一些次优解来跳出局部最优解。</p>
<p>应用：适用于解决组合优化问题，如旅行商问题(TSP)和调度问题。</p>
<p>6) <code>拉格朗日乘子法(Lagrange Multiplier Method)</code></p>
<p>原理：拉格朗日乘子法是一种求解约束优化问题的技术。通过引入拉格朗日乘子，将约束条件合并到目标函数中，从而转化为无约束问题进行优化。</p>
<p>应用：常用于带有等式约束的优化问题。高等数学里面有关于这部分的学习和使用，就不做介绍了。</p>
<p>7) <code>线性规划(Linear Programming, LP)</code></p>
<p>原理：线性规划求解的是具有线性目标函数和线性约束条件的优化问题。常用的求解方法有单纯形法和内点法。</p>
<p>应用：广泛应用于物流、生产调度、资源分配等领域。</p>
<p>用于求解线性规划类问题的工具个人推崇<code>Lingo</code>, 非常好用，Mathematica求解这类问题有大量丰富的函数，也强烈推荐使用。</p>
<p>8) <code>凸优化(Convex Optimization)</code></p>
<p>原理：凸优化问题是指目标函数是凸函数且约束条件也是凸的优化问题。凸优化理论提供了强大的数学工具，保证了全局最优解的存在和可求解性。</p>
<p>应用：许多工程和机器学习问题都可以建模为凸优化问题，如支持向量机(SVM)中的最优化问题。</p>
<p>9) <code>深度学习中的优化算法</code></p>
<p>原理：在神经网络和深度学习中，常用的优化算法有：</p>
<p>&bull; Adam：结合了动量法和自适应学习率的方法，常用于训练神经网络。</p>
<p>&bull; RMSprop：改进了梯度下降法，通过对历史梯度的平方进行加权平均来动态调整学习率。</p>
<p>&bull; Adagrad：根据梯度的平方和调整每个参数的学习率，适用于稀疏数据。</p>
<p>下面对用于工作中比较多的一些优化算法做一些相关的介绍。</p>
<h1 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h1><p>梯度下降法(英语：Gradient descent)是一个一阶最优化算法，通常也称为最陡下降法，但是不该与近似积分的最陡下降法(英语：Method of steepest descent)混淆。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度(或者是近似梯度)的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。</p>
<p>这里简要的说明下梯度的定义：</p>
<p><img src="https://latex.codecogs.com/svg.image?&space;grad&space;f(x_0,x_1,\dots,x_n)=(\frac{\partial&space;f}{\partial&space;x_0},\dots,\frac{\partial&space;f}{\partial&space;x_j},\dots,\frac{\partial&space;f}{\partial&space;x_n})" title=" grad f(x_0,x_1,\dots,x_n)=(\frac{\partial f}{\partial x_0},\dots,\frac{\partial f}{\partial x_j},\dots,\frac{\partial f}{\partial x_n})" /></p>
<p>函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。</p>
<p>这里注意三点：</p>
<p>&bull; 梯度是一个向量，即有方向有大小；<br>&bull; 梯度的方向是最大方向导数的方向；<br>&bull; 梯度的值是最大方向导数的值。</p>
<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>梯度下降方法基于以下的观察:如果实值函数F(x)在点a处可微且有定义,那么函数F(x)在a点沿着梯度相反的方向-∇F(a)下降最多。</p>
<p>因而，如果</p>
<p><img src="https://latex.codecogs.com/svg.image?\mathbf{b}=\mathbf{a}-\gamma\nabla&space;F(\mathbf{a})" title="\mathbf{b}=\mathbf{a}-\gamma\nabla F(\mathbf{a})" /></p>
<p>对于一个足够小数值<img src="https://latex.codecogs.com/svg.image?\gamma>0" title="\gamma>0" />时成立，那么<img src="https://latex.codecogs.com/svg.image?{\displaystyle&space;F(\mathbf{a})\geq&space;F(\mathbf{b})}" title="{\displaystyle F(\mathbf{a})\geq F(\mathbf{b})}" /></p>
<p>考虑到这一点，我们可以从函数 F 的局部极小值的初始估计 <img src="https://latex.codecogs.com/svg.image?\mathbf{x}_{0}" title="\mathbf{x}_{0}" /> 出发，并考虑如下序列 <img src="https://latex.codecogs.com/svg.image?{\displaystyle\mathbf{x}_{0},\mathbf{x}_{1},\mathbf{x}_{2},\dots}" title="{\displaystyle\mathbf{x}_{0},\mathbf{x}_{1},\mathbf{x}_{2},\dots}" /> 使得</p>
<p><img src="https://latex.codecogs.com/png.image?\dpi{110}\mathbf{x}_{n&plus;1}=\mathbf{x}_{n}-\gamma&space;_{n}\nabla&space;F(x_n),n\geq&space;0" title="\mathbf{x}_{n+1}=\mathbf{x}_{n}-\gamma _{n}\nabla F(x_n),n\geq 0" /></p>
<p>因此可得到</p>
<p><img src="https://latex.codecogs.com/png.image?\dpi{110}F(\mathbf{x}_{0})\geq&space;F(\mathbf{x}_{1})\geq&space;F(\mathbf{x}_{2})\geq\cdots" title="F(\mathbf{x}_{0})\geq F(\mathbf{x}_{1})\geq F(\mathbf{x}_{2})\geq\cdots" /></p>
<p>如果顺利的话序列  (<img src="https://latex.codecogs.com/png.image?\dpi{110}\mathbf{x}_{n}" title="\mathbf{x}_{n}" />)收敛到期望的局部极小值。注意每次迭代步长 <img src="https://latex.codecogs.com/png.image?\dpi{110}\gamma" title="\gamma" /> 可以改变。</p>
<p>下图示例了这一过程，这里假设 F 定义在平面上，并且函数图像是一个碗形。蓝色的曲线是等高线(水平集)，即函数 F 为常数的集合构成的曲线。红色的箭头指向该点梯度的反方向。(一点处的梯度方向与通过该点的等高线垂直)。沿着梯度下降方向，将最终到达碗底，即函数 F 局部极小值的点。 </p>
<p><img src="/img/postImg/16_Algorithm/2_常见的优化算法/1_梯度下降法/Gradient_descent.png" alt=""></p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>梯度下降法处理一些复杂的非线性函数会出现问题，例如Rosenbrock函数</p>
<p><img src="https://latex.codecogs.com/png.image?\dpi{110}{\displaystyle&space;f(x,y)=(1-x)^{2}&plus;100(y-x^{2})^{2}.\quad}" title="{\displaystyle f(x,y)=(1-x)^{2}+100(y-x^{2})^{2}.\quad}" /></p>
<p>其最小值在 (x,y)=(1,1) 处，数值为 f(x,y)=0。但是此函数具有狭窄弯曲的山谷，最小值 (x,y)=(1,1) 就在这些山谷之中，并且谷底很平。优化过程是之字形的向极小值点靠近，速度非常缓慢。如下图所示</p>
<p><img src="/img/postImg/16_Algorithm/2_常见的优化算法/1_梯度下降法/Banana-SteepDesc.gif" alt=""></p>
<h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>梯度下降法的缺点包括:</p>
<p>&bull; 靠近局部极小值时速度减慢。</p>
<p>&bull; 直线搜索可能会产生一些问题。</p>
<p>&bull; 可能会“之字型”地下降。</p>
<p>从上图可以看出，梯度下降法在接近最优解的区域收敛速度明显变慢，利用梯度下降法求解需要很多次的迭代。</p>
<p>在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。</p>
<p>比如对一个线性回归(Linear Logistics)模型，假设下面的h(x)是要拟合的函数，J(theta)为损失函数，theta是参数，要迭代求解的值，theta求解出来了那最终要拟合的函数h(theta)就出来了。其中m是训练集的样本个数，n是特征的个数。</p>
<p><img src="https://latex.codecogs.com/svg.image?&space;h(\theta)=\sum_{j=0}^n\theta_jx_j" title=" h(\theta)=\sum_{j=0}^n\theta_jx_j" /></p>
<p><img src="https://latex.codecogs.com/svg.image?J(\theta)=\frac{1}{2m}\sum_{i=1}^m(y^i-h_\theta(x^i))^2" title="J(\theta)=\frac{1}{2m}\sum_{i=1}^m(y^i-h_\theta(x^i))^2" /></p>
<h2 id="批量梯度下降法"><a href="#批量梯度下降法" class="headerlink" title="批量梯度下降法"></a>批量梯度下降法</h2><p>(1)将J(theta)对theta求偏导，得到每个theta对应的的梯度：</p>
<p><img src="https://latex.codecogs.com/svg.image?\frac{\partial&space;J(\theta)}{\partial\theta_j}=-\frac{1}{m}\sum_{i=1}^m(y^i-h_\theta(x^i))x^i_j" title="\frac{\partial J(\theta)}{\partial\theta_j}=-\frac{1}{m}\sum_{i=1}^m(y^i-h_\theta(x^i))x^i_j" /></p>
<p>(2)由于是要最小化风险函数，所以按每个参数theta的梯度负方向，来更新每个theta：</p>
<p><img src="https://latex.codecogs.com/svg.image?{\theta_j}^{'}=\theta_j&plus;\frac{1}{m}\sum_{i=1}^m(y^i-h_\theta(x^i))x_j^i" title="{\theta_j}^{'}=\theta_j+\frac{1}{m}\sum_{i=1}^m(y^i-h_\theta(x^i))x_j^i" /></p>
<p>(3)从上面公式可以注意到，它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果m很大，那么可想而知这种方法的迭代速度会相当的慢。所以，这就引入了另外一种方法——随机梯度下降。</p>
<p>　　对于批量梯度下降法，样本个数m，x为n维向量，一次迭代需要把m个样本全部带入计算，迭代一次计算量为<img src="https://latex.codecogs.com/svg.image?m*n^2" title="m*n^2" />。</p>
<h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>(1)上面的风险函数可以写成如下这种形式，损失函数对应的是训练集中每个样本的粒度，而上面批量梯度下降对应的是所有的训练样本：</p>
<p><img src="https://latex.codecogs.com/svg.image?&space;J(\theta)=\frac{1}{m}\sum_{i=1}^m\frac{1}{2}(y^i-h\theta(x^i))^2=\frac{1}{m}\sum_{i=1}^mcost(\theta,(x^i,y^i))" title=" J(\theta)=\frac{1}{m}\sum_{i=1}^m\frac{1}{2}(y^i-h\theta(x^i))^2=\frac{1}{m}\sum_{i=1}^mcost(\theta,(x^i,y^i))" /></p>
<p><img src="https://latex.codecogs.com/svg.image?cost(\theta,(x^i,y^i))=\frac{1}{2}(y^i-h_\theta(x^i))^2" title="cost(\theta,(x^i,y^i))=\frac{1}{2}(y^i-h_\theta(x^i))^2" /></p>
<p>(2)每个样本的损失函数，对theta求偏导得到对应梯度，来更新theta：</p>
<p><img src="https://latex.codecogs.com/svg.image?\theta_j^{'}=\theta_j&plus;(y^i-h\theta(x^i))x_j^i" title="\theta_j^{'}=\theta_j+(y^i-h\theta(x^i))x_j^i" /></p>
<p>(3)随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况(例如几十万)，那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p>
<p>　　随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n2，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。<strong>两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。</strong></p>
<h1 id="牛顿法和拟牛顿法"><a href="#牛顿法和拟牛顿法" class="headerlink" title="牛顿法和拟牛顿法"></a>牛顿法和拟牛顿法</h1><h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p><strong>牛顿法</strong>(英语：Newton’s method)又称为<strong>牛顿-拉弗森方法</strong>(英语：Newton-Raphson method)，它是一种在实数域和复数域上近似求解方程的方法。方法使用函数f(x)的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/泰勒级数">泰勒级数</a>的前面几项来寻找方程<code>f(x)=0</code>的根。</p>
<h3 id="方法说明"><a href="#方法说明" class="headerlink" title="方法说明"></a>方法说明</h3><p>首先,选择一个接近函数f(x)零点的x0,计算相应的f(x0)和切线斜率f’(x0)(这里f表示函数f的导数)。然后我们计算穿过点(x0,f(x0))并且斜率为f’(x0)的直线和x轴的交点的x坐标,也就是求如下方程的解:</p>
<p><img src="https://latex.codecogs.com/svg.image?0=(x-x_0)\cdot&space;f^{'}(x_0)&plus;f(x_0)" title="0=(x-x_0)\cdot f^{'}(x_0)+f(x_0)" /></p>
<p>我们将新求得的点的x坐标命名为x1,通常x1会比x0更接近方程f(x)=0的解。因此我们现在可以利用x1开始下一轮迭代。迭代公式可化简为如下所示:</p>
<p><img src="https://latex.codecogs.com/svg.image?x_{n&plus;1}=x_n-\frac{f(x_n)}{f^{'}(x_n)}" title="x_{n+1}=x_n-\frac{f(x_n)}{f^{'}(x_n)}" /></p>
<p>然而当f(x)=0在x=a处有m重根时,这时牛顿法会降为线性收敛,虽然使用牛顿法也可以继续算下去,但收敛速度会减慢。</p>
<p><img src="/img/postImg/16_Algorithm/2_常见的优化算法/1_梯度下降法/600px-NewtonIteration_Ani.gif" alt=""></p>
<p>　　</p>
<h3 id="牛顿法的优缺点总结："><a href="#牛顿法的优缺点总结：" class="headerlink" title="牛顿法的优缺点总结："></a>牛顿法的优缺点总结：</h3><p>&bull; 优点：二阶收敛，收敛速度快；</p>
<p>&bull; 缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。</p>
<h2 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h2><p>拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p>
<p><strong>拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。</strong></p>
<h3 id="搜索极值"><a href="#搜索极值" class="headerlink" title="搜索极值"></a>搜索极值</h3><p>与牛顿法相同,拟牛顿法是用一个二次函数以近似目标函数f(x). f(x)的阶泰勒展开是</p>
<p><img src="https://latex.codecogs.com/svg.image?x_k{\displaystyle&space;f(x_{k}&plus;\Delta&space;x)\approx&space;f(x_{k})&plus;\nabla&space;f(x_{k})^{T}\Delta&space;x&plus;{\frac{1}{2}}\Delta&space;x^{T}B\Delta&space;x.}" title="x_k{\displaystyle f(x_{k}+\Delta x)\approx f(x_{k})+\nabla f(x_{k})^{T}\Delta x+{\frac{1}{2}}\Delta x^{T}B\Delta x.}" /></p>
<p>其中,  <img src="https://latex.codecogs.com/svg.image?\nabla&space;f" title="\nabla f" />表示f(x)的梯度,B表示Hessian矩阵<strong>H[f(x)]</strong>的近似.梯度<img src="https://latex.codecogs.com/svg.image?\nabla&space;f" title="\nabla f" />可进一步近似为下列形式</p>
<p><img src="https://latex.codecogs.com/svg.image?{\displaystyle\nabla&space;f(x_{k}&plus;\Delta&space;x)\approx\nabla&space;f(x_{k})&plus;B\Delta&space;x.}" title="{\displaystyle\nabla f(x_{k}+\Delta x)\approx\nabla f(x_{k})+B\Delta x.}" /></p>
<p>令上式等于0,计算出Newton步长△x,</p>
<p><img src="https://latex.codecogs.com/svg.image?{\displaystyle\Delta&space;x=-B^{-1}\nabla&space;f(x_{k}).}" title="{\displaystyle\Delta x=-B^{-1}\nabla f(x_{k}).}" /></p>
<p>然后构造<strong>H[f(x)]</strong>的近似B满足</p>
<p><img src="https://latex.codecogs.com/svg.image?{\displaystyle\nabla&space;f(x_{k}&plus;\Delta&space;x)=\nabla&space;f(x_{k})&plus;B\Delta&space;x.}" title="{\displaystyle\nabla f(x_{k}+\Delta x)=\nabla f(x_{k})+B\Delta x.}" /></p>
<p>以下方式更新B</p>
<p><img src="https://latex.codecogs.com/svg.image?{\displaystyle&space;B_{k&plus;1}=\arg\min&space;_{B}\|B-B_{k}\|_{V}.}" title="{\displaystyle B_{k+1}=\arg\min _{B}\|B-B_{k}\|_{V}.}" /></p>
<p>近似<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Hessian矩阵">Hessian矩阵</a>一般以单位矩阵等作为初期值. 最优化问题的解<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6d2b88c64c76a03611549fb9b4cf4ed060b56002" alt="{\displaystyle x_{k}}">由根据近似所得的<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6a6457760e36cf45e1471e33bcc1536cb4802fb9" alt="{\displaystyle B_{k}}">计算出的Newton步长更新得出.</p>
<p>以下为该算法的总结:</p>
<p>&bull; <img src="https://latex.codecogs.com/svg.image?{\displaystyle\Delta&space;x_{k}=-\alpha&space;B_{k}^{-1}\nabla&space;f(x_{k})}" title="{\displaystyle\Delta x_{k}=-\alpha B_{k}^{-1}\nabla f(x_{k})}" /></p>
<p>&bull; <img src="https://latex.codecogs.com/svg.image?{\displaystyle&space;x_{k&plus;1}=x_{k}&plus;\Delta&space;x_{k}}" title="{\displaystyle x_{k+1}=x_{k}+\Delta x_{k}}" /></p>
<p>&bull; 计算新一个迭代点下的梯度<img src="https://latex.codecogs.com/svg.image?{\displaystyle\nabla&space;f(x_{k&plus;1})}" title="{\displaystyle\nabla f(x_{k+1})}" /></p>
<p>&bull; 令<img src="https://latex.codecogs.com/svg.image?{\displaystyle&space;y_{k}=\nabla&space;f(x_{k&plus;1})-\nabla&space;f(x_{k})}" title="{\displaystyle y_{k}=\nabla f(x_{k+1})-\nabla f(x_{k})}" /></p>
<p>&bull; 利用<img src="https://latex.codecogs.com/svg.image?{\displaystyle&space;y_{k}}" title="{\displaystyle y_{k}}" />, 直接近似Hessian矩阵的逆矩阵<img src="https://latex.codecogs.com/svg.image?{\displaystyle&space;B_{k&plus;1}^{-1}}" title="{\displaystyle B_{k+1}^{-1}}" />. 近似的方法如下表:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Method</th>
<th style="text-align:center"><img src="https://latex.codecogs.com/svg.image?{\displaystyle\displaystyle&space;B_{k&plus;1}=}" title="{\displaystyle\displaystyle B_{k+1}=}" /></th>
<th style="text-align:center"><img src="https://latex.codecogs.com/svg.image?{\displaystyle&space;H_{k&plus;1}=B_{k&plus;1}^{-1}=}" title="{\displaystyle H_{k+1}=B_{k+1}^{-1}=}" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">DFP法</td>
<td style="text-align:center"><img src="https://latex.codecogs.com/svg.image?{\displaystyle\left(I-{\frac{y_{k}\,\Delta&space;x_{k}^{T}}{y_{k}^{T}\,\Delta&space;x_{k}}}\right)B_{k}\left(I-{\frac{\Delta&space;x_{k}y_{k}^{T}}{y_{k}^{T}\,\Delta&space;x_{k}}}\right)&plus;{\frac{y_{k}y_{k}^{T}}{y_{k}^{T}\,\Delta&space;x_{k}}}" title="{\displaystyle\left(I-{\frac{y_{k}\,\Delta x_{k}^{T}}{y_{k}^{T}\,\Delta x_{k}}}\right)B_{k}\left(I-{\frac{\Delta x_{k}y_{k}^{T}}{y_{k}^{T}\,\Delta x_{k}}}\right)+{\frac{y_{k}y_{k}^{T}}{y_{k}^{T}\,\Delta x_{k}}}" /></td>
<td style="text-align:center"><img src="https://latex.codecogs.com/svg.image?{\displaystyle&space;H_{k}&plus;{\frac{\Delta&space;x_{k}\Delta&space;x_{k}^{T}}{y_{k}^{T}\,\Delta&space;x_{k}}}-{\frac{H_{k}y_{k}y_{k}^{T}H_{k}^{T}}{y_{k}^{T}H_{k}y_{k}}}" title="{\displaystyle H_{k}+{\frac{\Delta x_{k}\Delta x_{k}^{T}}{y_{k}^{T}\,\Delta x_{k}}}-{\frac{H_{k}y_{k}y_{k}^{T}H_{k}^{T}}{y_{k}^{T}H_{k}y_{k}}}" /></td>
</tr>
<tr>
<td style="text-align:center">BFGS法</td>
<td style="text-align:center"><img src="https://latex.codecogs.com/svg.image?{\displaystyle&space;B_{k}&plus;{\frac{y_{k}y_{k}^{T}}{y_{k}^{T}\Delta&space;x_{k}}}-{\frac{B_{k}\Delta&space;x_{k}(B_{k}\Delta&space;x_{k})^{T}}{\Delta&space;x_{k}^{T}B_{k}\,\Delta&space;x_{k}}}}" title="{\displaystyle B_{k}+{\frac{y_{k}y_{k}^{T}}{y_{k}^{T}\Delta x_{k}}}-{\frac{B_{k}\Delta x_{k}(B_{k}\Delta x_{k})^{T}}{\Delta x_{k}^{T}B_{k}\,\Delta x_{k}}}}" /></td>
<td style="text-align:center"><img src="https://latex.codecogs.com/svg.image?{\displaystyle\left(I-{\frac{y_{k}\Delta&space;x_{k}^{T}}{y_{k}^{T}\Delta&space;x_{k}}}\right)^{T}H_{k}\left(I-{\frac{y_{k}\Delta&space;x_{k}^{T}}{y_{k}^{T}\Delta&space;x_{k}}}\right)&plus;{\frac{\Delta&space;x_{k}\Delta&space;x_{k}^{T}}{y_{k}^{T}\,\Delta&space;x_{k}}}" title="{\displaystyle\left(I-{\frac{y_{k}\Delta x_{k}^{T}}{y_{k}^{T}\Delta x_{k}}}\right)^{T}H_{k}\left(I-{\frac{y_{k}\Delta x_{k}^{T}}{y_{k}^{T}\Delta x_{k}}}\right)+{\frac{\Delta x_{k}\Delta x_{k}^{T}}{y_{k}^{T}\,\Delta x_{k}}}" /></td>
</tr>
<tr>
<td style="text-align:center">Broyden法</td>
<td style="text-align:center"><img src="https://latex.codecogs.com/svg.image?{\displaystyle\left(I-{\frac{y_{k}\Delta&space;x_{k}^{T}}{y_{k}^{T}\Delta&space;x_{k}}}\right)^{T}H_{k}\left(I-{\frac{y_{k}\Delta&space;x_{k}^{T}}{y_{k}^{T}\Delta&space;x_{k}}}\right)&plus;{\frac{\Delta&space;x_{k}\Delta&space;x_{k}^{T}}{y_{k}^{T}\,\Delta&space;x_{k}}}{\displaystyle&space;B_{k}&plus;{\frac{y_{k}-B_{k}\Delta&space;x_{k}}{\Delta&space;x_{k}^{T}\,\Delta&space;x_{k}}}\,\Delta&space;x_{k}^{T}" title="{\displaystyle\left(I-{\frac{y_{k}\Delta x_{k}^{T}}{y_{k}^{T}\Delta x_{k}}}\right)^{T}H_{k}\left(I-{\frac{y_{k}\Delta x_{k}^{T}}{y_{k}^{T}\Delta x_{k}}}\right)+{\frac{\Delta x_{k}\Delta x_{k}^{T}}{y_{k}^{T}\,\Delta x_{k}}}{\displaystyle B_{k}+{\frac{y_{k}-B_{k}\Delta x_{k}}{\Delta x_{k}^{T}\,\Delta x_{k}}}\,\Delta x_{k}^{T}" /></td>
<td style="text-align:center"><img src="https://latex.codecogs.com/svg.image?{\displaystyle&space;H_{k}&plus;{\frac{(\Delta&space;x_{k}-H_{k}y_{k})\Delta&space;x_{k}^{T}H_{k}}{\Delta&space;x_{k}^{T}H_{k}\,y_{k}}}" title="{\displaystyle H_{k}+{\frac{(\Delta x_{k}-H_{k}y_{k})\Delta x_{k}^{T}H_{k}}{\Delta x_{k}^{T}H_{k}\,y_{k}}}" /></td>
</tr>
<tr>
<td style="text-align:center">Broyden族</td>
<td style="text-align:center"><img src="https://latex.codecogs.com/svg.image?{\displaystyle(1-\varphi&space;_{k})B_{k&plus;1}^{BFGS}&plus;\varphi&space;_{k}B_{k&plus;1}^{DFP},\qquad\varphi\in[0,1]}" title="{\displaystyle(1-\varphi _{k})B_{k+1}^{BFGS}+\varphi _{k}B_{k+1}^{DFP},\qquad\varphi\in[0,1]}" /></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">SR1法</td>
<td style="text-align:center"><img src="https://latex.codecogs.com/svg.image?{\displaystyle&space;B_{k}&plus;{\frac{(y_{k}-B_{k}\,\Delta&space;x_{k})(y_{k}-B_{k}\,\Delta&space;x_{k})^{T}}{(y_{k}-B_{k}\,\Delta&space;x_{k})^{T}\,\Delta&space;x_{k}}}}" title="{\displaystyle B_{k}+{\frac{(y_{k}-B_{k}\,\Delta x_{k})(y_{k}-B_{k}\,\Delta x_{k})^{T}}{(y_{k}-B_{k}\,\Delta x_{k})^{T}\,\Delta x_{k}}}}" /></td>
<td style="text-align:center"><img src="https://latex.codecogs.com/svg.image?{\displaystyle&space;H_{k}&plus;{\frac{(\Delta&space;x_{k}-H_{k}y_{k})(\Delta&space;x_{k}-H_{k}y_{k})^{T}}{(\Delta&space;x_{k}-H_{k}y_{k})^{T}y_{k}}}}" title="{\displaystyle H_{k}+{\frac{(\Delta x_{k}-H_{k}y_{k})(\Delta x_{k}-H_{k}y_{k})^{T}}{(\Delta x_{k}-H_{k}y_{k})^{T}y_{k}}}}" /></td>
</tr>
</tbody>
</table>
</div>
<h1 id="共轭梯度法"><a href="#共轭梯度法" class="headerlink" title="共轭梯度法"></a>共轭梯度法</h1><p><strong>共轭梯度法</strong>(英语：Conjugate gradient method)，是求解系数矩阵为对称正定矩阵的线性方程组的数值解的方法。共轭梯度法是一个迭代方法，它适用于系数矩阵为稀疏矩阵的线性方程组，因为使用像Cholesky分解这样的直接方法求解这些系统所需的计算量太大了。这种方程组在数值求解偏微分方程时很常见。</p>
<p>共轭梯度法也可以用于求解无约束的最优化问题。</p>
<h2 id="方法的表述"><a href="#方法的表述" class="headerlink" title="方法的表述"></a>方法的表述</h2><p>要求解下列线性系统</p>
<p><img src="https://latex.codecogs.com/svg.image?Ax=b&space;" title="Ax=b " /></p>
<p>其中 nxn 矩阵 A 是对称的，正定的，并且是实系数的。 将系统的唯一解记作 x∗</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>经过一些简化，可以得到下列求解<img src="https://latex.codecogs.com/svg.image?Ax=b&space;" title="Ax=b " />的算法，其中A是实对称正定矩阵。</p>
<p><img src="/img/postImg/16_Algorithm/2_常见的优化算法/1_梯度下降法/共轭梯度法.svg" alt=""></p>
<h1 id="序列二次规划"><a href="#序列二次规划" class="headerlink" title="序列二次规划"></a>序列二次规划</h1><p>序列二次规划(Sequential Quadratic Programming，SQP)是一类用于解决非线性优化问题的迭代方法，广泛应用于约束优化问题，尤其是在控制、机器人学和工程设计中。SQP方法通过将非线性问题转化为一系列二次规划问题来逐步逼近最优解。它的基本思想是在每次迭代中通过求解一个二次优化问题，找到一个改进方向，并在此方向上更新当前解。</p>
<h2 id="SQP方法的基本步骤"><a href="#SQP方法的基本步骤" class="headerlink" title="SQP方法的基本步骤"></a>SQP方法的基本步骤</h2><h3 id="目标函数和约束的泰勒展开"><a href="#目标函数和约束的泰勒展开" class="headerlink" title="目标函数和约束的泰勒展开"></a>目标函数和约束的泰勒展开</h3><p>&bull; 对于给定的非线性目标函数和约束条件，SQP方法首先对目标函数和约束条件在当前点进行一阶(梯度)和二阶(Hessian矩阵)泰勒展开。</p>
<h3 id="构造二次规划子问题"><a href="#构造二次规划子问题" class="headerlink" title="构造二次规划子问题"></a>构造二次规划子问题</h3><p>&bull; 使用当前点的信息(包括目标函数的梯度和约束的雅可比矩阵、Hessian矩阵等)来构造一个近似的二次规划问题(QP)。</p>
<p>&bull; 该QP问题通常形如</p>
<p><img src="https://latex.codecogs.com/svg.image?\min\limits_{\delta&space;x}\frac{1}{2}\delta&space;x^TH_k\delta&space;x&plus;g_k^T\delta&space;x&space;" title="\min\limits_{\delta x}\frac{1}{2}\delta x^TH_k\delta x+g_k^T\delta x " /></p>
<p>&bull; 其中，Hk是目标函数的Hessian矩阵的近似，gk是目标函数梯度的近似，δx是当前解的改进方向。</p>
<h3 id="解二次规划子问题"><a href="#解二次规划子问题" class="headerlink" title="解二次规划子问题"></a>解二次规划子问题</h3><p>&bull; 通过求解上面构造的QP子问题，得到一个搜索方向(即改进方向)δx，并更新当前解： </p>
<p><img src="https://latex.codecogs.com/svg.image?x_{k&plus;1}=x_k&plus;\delta&space;x_k&space;" title="x_{k+1}=x_k+\delta x_k " /></p>
<h3 id="判断收敛性"><a href="#判断收敛性" class="headerlink" title="判断收敛性"></a>判断收敛性</h3><p>&bull; 判断算法是否满足收敛准则(例如目标函数的变化足够小、梯度足够接近零、约束条件得到满足等)。如果收敛条件满足，则终止；否则，返回步骤1继续迭代。</p>
<h3 id="约束处理"><a href="#约束处理" class="headerlink" title="约束处理"></a>约束处理</h3><p>&bull; SQP方法通过引入拉格朗日乘子来处理约束问题。约束可以是等式约束或不等式约束，常见的约束类型包括线性约束和非线性约束。</p>
<h2 id="SQP的优点"><a href="#SQP的优点" class="headerlink" title="SQP的优点"></a>SQP的优点</h2><p>&bull; 高效性: SQP在大多数优化问题中表现出较快的收敛速度，尤其适用于具有光滑目标函数和约束的非线性优化问题。</p>
<p>&bull; 适应性: 能够处理各种类型的约束(包括等式约束、不等式约束、边界约束等)。</p>
<p>&bull; 精确性: 由于每一步都通过求解一个二次规划问题，它能较精确地逼近局部最优解。</p>
<h2 id="SQP的缺点"><a href="#SQP的缺点" class="headerlink" title="SQP的缺点"></a>SQP的缺点</h2><p>&bull; 计算成本高: 每次迭代都需要求解一个二次规划问题，这可能需要较高的计算资源，尤其是在问题规模较大时。</p>
<p>&bull; 需要二阶导数信息: SQP需要目标函数和约束的Hessian矩阵(或其近似)，这在某些情况下可能难以获得，或者计算复杂。</p>
<p>&bull; 对初始值敏感: SQP对初始解有一定的依赖性，若初始解距离最优解较远，可能会收敛到局部最优解。</p>
<h2 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h2><p>SQP方法在许多领域中得到了广泛应用，特别是以下几类问题：</p>
<p>&bull; 机器人学中的路径规划和轨迹优化:SQP常用于求解具有非线性约束的运动规划问题，如机器人路径的优化。</p>
<p>&bull; 控制理论: SQP用于求解最优控制问题，尤其是涉及非线性动力学和约束的控制问题。</p>
<p>&bull; 结构优化: 在工程设计中，SQP可用于结构优化问题，例如材料分布的优化。</p>
<p>&bull; 经济学和金融中的最优决策: SQP也可用于经济学和金融领域中的非线性最优化问题，如投资组合优化。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] chatGPT</p>
<p>[2] <a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95">维基百科</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://www.cnblogs.com/xinbaby829/p/7289431.html">几种常见的优化算法</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%89%9B%E9%A1%BF%E6%B3%95">牛顿法</a></p>

            </div>



</div>

   <div class="tag-cloud-tags">
    <a href="/tags/CMakeLists/" style="font-size: 10px; color: #f38181">CMakeLists</a> <a href="/tags/Eigen/" style="font-size: 10px; color: #f38181">Eigen</a> <a href="/tags/FCPX/" style="font-size: 10px; color: #f38181">FCPX</a> <a href="/tags/GNU/" style="font-size: 10px; color: #f38181">GNU</a> <a href="/tags/Gazebo/" style="font-size: 10px; color: #f38181">Gazebo</a> <a href="/tags/Git/" style="font-size: 10px; color: #f38181">Git</a> <a href="/tags/Interest/" style="font-size: 10px; color: #f38181">Interest</a> <a href="/tags/KDL/" style="font-size: 11px; color: #ea8b89">KDL</a> <a href="/tags/Life/" style="font-size: 10px; color: #f38181">Life</a> <a href="/tags/Linux/" style="font-size: 11px; color: #ea8b89">Linux</a> <a href="/tags/Matrix/" style="font-size: 10px; color: #f38181">Matrix</a> <a href="/tags/ODE/" style="font-size: 10px; color: #f38181">ODE</a> <a href="/tags/ROS/" style="font-size: 13px; color: #d79e9a">ROS</a> <a href="/tags/Ros/" style="font-size: 10px; color: #f38181">Ros</a> <a href="/tags/UML/" style="font-size: 10px; color: #f38181">UML</a> <a href="/tags/Ubuntu/" style="font-size: 10px; color: #f38181">Ubuntu</a> <a href="/tags/VcXsrv/" style="font-size: 10px; color: #f38181">VcXsrv</a> <a href="/tags/algorithm/" style="font-size: 10px; color: #f38181">algorithm</a> <a href="/tags/algorithms/" style="font-size: 10px; color: #f38181">algorithms</a> <a href="/tags/axis-angle/" style="font-size: 10px; color: #f38181">axis-angle</a> <a href="/tags/bode/" style="font-size: 10px; color: #f38181">bode</a> <a href="/tags/calibration/" style="font-size: 10px; color: #f38181">calibration</a> <a href="/tags/chrome/" style="font-size: 10px; color: #f38181">chrome</a> <a href="/tags/control/" style="font-size: 14px; color: #cda7a2">control</a> <a href="/tags/cpp/" style="font-size: 18px; color: #a8cec3">cpp</a> <a href="/tags/dB/" style="font-size: 10px; color: #f38181">dB</a> <a href="/tags/data-struct/" style="font-size: 10px; color: #f38181">data_struct</a> <a href="/tags/dots/" style="font-size: 16px; color: #bbbbb2">dots</a> <a href="/tags/figure/" style="font-size: 10px; color: #f38181">figure</a> <a href="/tags/gdb/" style="font-size: 10px; color: #f38181">gdb</a> <a href="/tags/latex/" style="font-size: 11px; color: #ea8b89">latex</a> <a href="/tags/launch/" style="font-size: 10px; color: #f38181">launch</a> <a href="/tags/life/" style="font-size: 17px; color: #b1c4ba">life</a> <a href="/tags/linux/" style="font-size: 18px; color: #a8cec3">linux</a> <a href="/tags/mac/" style="font-size: 10px; color: #f38181">mac</a> <a href="/tags/math/" style="font-size: 14px; color: #cda7a2">math</a> <a href="/tags/matlab/" style="font-size: 12px; color: #e09491">matlab</a> <a href="/tags/memory/" style="font-size: 15px; color: #c4b1aa">memory</a> <a href="/tags/motor/" style="font-size: 11px; color: #ea8b89">motor</a> <a href="/tags/moveit/" style="font-size: 10px; color: #f38181">moveit</a> <a href="/tags/operator/" style="font-size: 10px; color: #f38181">operator</a> <a href="/tags/optimal-algorithm/" style="font-size: 10px; color: #f38181">optimal algorithm</a> <a href="/tags/python/" style="font-size: 16px; color: #bbbbb2">python</a> <a href="/tags/robot/" style="font-size: 13px; color: #d79e9a">robot</a> <a href="/tags/robotics/" style="font-size: 13px; color: #d79e9a">robotics</a> <a href="/tags/ros/" style="font-size: 15px; color: #c4b1aa">ros</a> <a href="/tags/ros2/" style="font-size: 11px; color: #ea8b89">ros2</a> <a href="/tags/rtb/" style="font-size: 10px; color: #f38181">rtb</a> <a href="/tags/simulation/" style="font-size: 11px; color: #ea8b89">simulation</a> <a href="/tags/stl/" style="font-size: 10px; color: #f38181">stl</a> <a href="/tags/thread/" style="font-size: 10px; color: #f38181">thread</a> <a href="/tags/tools/" style="font-size: 19px; color: #9ed7cb">tools</a> <a href="/tags/twist/" style="font-size: 10px; color: #f38181">twist</a> <a href="/tags/urdf/" style="font-size: 10px; color: #f38181">urdf</a> <a href="/tags/velocity/" style="font-size: 10px; color: #f38181">velocity</a> <a href="/tags/vim/" style="font-size: 14px; color: #cda7a2">vim</a> <a href="/tags/web/" style="font-size: 10px; color: #f38181">web</a> <a href="/tags/work/" style="font-size: 20px; color: #95e1d3">work</a> <a href="/tags/wsl/" style="font-size: 10px; color: #f38181">wsl</a>
</div>



<div align="center"> 
    <li id="footer-info-copyright">网站内容采用<a class="external" rel="nofollow noopener" target="_blank" href="https://www.gnu.org/copyleft/fdl.html">GNU自由文档许可证3或更高版本</a>授权</li> <img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png" alt="知识共享许可协议">



        </div>
    </div>
</body>


</html>
